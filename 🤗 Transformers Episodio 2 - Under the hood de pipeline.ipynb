{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d3281b",
   "metadata": {},
   "source": [
    "# ðŸ¤— Transformers Episodio 2 - Under the hood de pipeline\n",
    "\n",
    "[twitch.tv/dataista0](http://twitch.tv/dataista0)\n",
    "\n",
    "\n",
    "* [Lista de modelos](https://huggingface.co/transformers/pretrained_models.html)\n",
    "* [Quicktour](https://huggingface.co/transformers/quicktour.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d97bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb2f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e8fbd9592a458ca9915fa371fbd0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbf6814611f44acbfd82fb253e4d13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3317d01e6a4a54b3ce68ff286aaffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cc1d39d2fe46c0a3bea2ab5464e9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225456e98c4947b28d4c5ebb2777aa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m1 = pipeline('sentiment-analysis', framework=\"pt\", model=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12727db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5455528497695923}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1(\"I am very very sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8c9029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5358377695083618}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1(\"I am very very happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc283ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = pipeline('sentiment-analysis', framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec374c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b5e499f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dataista/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/__init__.py'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ca4a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function transformers.pipelines.pipeline(task: str, model: Optional = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, framework: Union[str, NoneType] = None, revision: Union[str, NoneType] = None, use_fast: bool = True, use_auth_token: Union[str, bool, NoneType] = None, model_kwargs: Dict[str, Any] = {'use_auth_token': None}, **kwargs) -> transformers.pipelines.base.Pipeline>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8abda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643a8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa516b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = pipeline(\"sentiment-analysis\", framework=\"pt\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "824cd8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995039105415344}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3(\"I am very very sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed4da4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998809099197388}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3(\"I am very very happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1740fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998473525047302}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3(\"I am partially happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee308b09",
   "metadata": {},
   "source": [
    "Sabe labels POSITIVE y NEGATIVE porque `SST-2` es un dataset de sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb133ec",
   "metadata": {},
   "source": [
    "# AutoModel y AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0e5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf48e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539c3715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.8320968151092529}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am very very very very happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ddeaad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.5637515783309937}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Mediocre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c02d905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.586713433265686}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Mediocre at most\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66e370f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.8148958086967468}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"El peor hotel que conocÃ­ en mi vida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e7e36ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7797722220420837}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Estoy muy muy feliz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6fb6852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.5999898314476013}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Estoy muy muy feliz de que la pelÃ­cula haya terminado. La peor en dÃ©cadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006b175",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Profundizar en tokenizer:\n",
    "* [Preprocessing](https://huggingface.co/transformers/preprocessing.html)\n",
    "* [Summary of the tokenizer](https://huggingface.co/transformers/tokenizer_summary.html)\n",
    "\n",
    "Carpeta de downloads: `~/.cache/huggingface/transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcae9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['TRANSFORMERS_CACHE'] = '/media/dataista/DATA/transformers'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89aefad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6eb190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66b8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The second wave of Covid seems to be finishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241b6a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 10103, 10981, 21560, 10108, 10348, 41194, 32681, 10114, 10346, 33300, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23acbe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 10103,\n",
       " 10981,\n",
       " 21560,\n",
       " 10108,\n",
       " 10348,\n",
       " 41194,\n",
       " 32681,\n",
       " 10114,\n",
       " 10346,\n",
       " 33300,\n",
       " 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7472a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa8be07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e74d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"The second wave of Covid seems to be finishing\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121610be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cafe4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\", \"Text\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    #return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d89bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  11312,\n",
       "  10320,\n",
       "  12495,\n",
       "  19308,\n",
       "  10114,\n",
       "  11391,\n",
       "  10855,\n",
       "  10103,\n",
       "  100,\n",
       "  58263,\n",
       "  13299,\n",
       "  119,\n",
       "  102],\n",
       " [101, 11312, 18763, 10855, 11530, 112, 162, 39487, 10197, 119, 102, 0, 0, 0],\n",
       " [101, 14059, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3194eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], [101, 11312, 18763, 10855, 11530, 112, 162, 39487, 10197, 119, 102, 0, 0, 0], [101, 14059, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for key, value in pt_batch.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f947cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa154bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(**{'a': 1, 'b': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "951d3615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>11312</td>\n",
       "      <td>10320</td>\n",
       "      <td>12495</td>\n",
       "      <td>19308</td>\n",
       "      <td>10114</td>\n",
       "      <td>11391</td>\n",
       "      <td>10855</td>\n",
       "      <td>10103</td>\n",
       "      <td>100</td>\n",
       "      <td>58263</td>\n",
       "      <td>13299</td>\n",
       "      <td>119</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>11312</td>\n",
       "      <td>18763</td>\n",
       "      <td>10855</td>\n",
       "      <td>11530</td>\n",
       "      <td>112</td>\n",
       "      <td>162</td>\n",
       "      <td>39487</td>\n",
       "      <td>10197</td>\n",
       "      <td>119</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>14059</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2      3      4      5      6      7      8    9      10  \\\n",
       "0  101  11312  10320  12495  19308  10114  11391  10855  10103  100  58263   \n",
       "1  101  11312  18763  10855  11530    112    162  39487  10197  119    102   \n",
       "2  101  14059    102      0      0      0      0      0      0    0      0   \n",
       "\n",
       "      11   12   13  \n",
       "0  13299  119  102  \n",
       "1      0    0    0  \n",
       "2      0    0    0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(pt_batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8118bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13\n",
       "0   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
       "1   1   1   1   1   1   1   1   1   1   1   1   0   0   0\n",
       "2   1   1   1   0   0   0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pt_batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb6eda",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "* [Model outputs](https://huggingface.co/transformers/main_classes/output.html)\n",
    "* [Fine tuning a pretrained model](https://huggingface.co/transformers/training.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92fa0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\", \n",
    "     \"El hotel era horrendo.\",\n",
    "     \"O hotel era esplÃªndido.\"\n",
    "    ],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9380402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = model(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd8ee486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6222, -2.7745, -0.8967,  2.0137,  3.3064],\n",
       "        [ 0.0064, -0.1258, -0.0503, -0.1655,  0.1329],\n",
       "        [ 2.1701,  1.8844,  0.6988, -1.5086, -2.4908],\n",
       "        [-1.6145, -1.3498,  0.1185,  1.0354,  1.4689]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db4cfde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df1b2ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.6100000143051147,\n",
       " -1.350000023841858,\n",
       " 0.11999999731779099,\n",
       " 1.0399999618530273,\n",
       " 1.4700000286102295]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs.logits[3].detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5560902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "pt_predictions = F.softmax(pt_outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d5667a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
       "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365],\n",
       "        [0.4961, 0.3728, 0.1139, 0.0125, 0.0047],\n",
       "        [0.0228, 0.0297, 0.1287, 0.3220, 0.4968]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65ff6128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 0, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_predictions.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0c1d7",
   "metadata": {},
   "source": [
    "# Grabar modelo y tokenizer finetuneados a disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc74b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0cccc43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/media/dataista/DATA/my-transformers/episodio-2/\"\n",
    "tokenizer.save_pretrained(save_directory+\"tokenizer/\")\n",
    "model.save_pretrained(save_directory+\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99680f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /media/dataista/DATA/my-transformers/episodio-2/tokenizer/config.json not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load config for '/media/dataista/DATA/my-transformers/episodio-2/tokenizer/'. Make sure that:\n",
      "\n",
      "- '/media/dataista/DATA/my-transformers/episodio-2/tokenizer/' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or '/media/dataista/DATA/my-transformers/episodio-2/tokenizer/' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No puedo gra\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory+\"tokenizer/\")\n",
    "    model = AutoModel.from_pretrained(save_directory+\"model/\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00c41555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf $/media/dataista/DATA/my-transformers/episodio-2/*\r\n"
     ]
    }
   ],
   "source": [
    "!echo rm -rf ${save_directory}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b938a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ${save_directory}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48c37ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2438056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /media/dataista/DATA/my-transformers/episodio-2/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModel.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32756a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs2 = model(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7645b589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3566, -0.4698, -0.2348,  ...,  0.9312,  0.0610, -0.2415],\n",
       "         [ 0.2775, -0.5162, -0.1987,  ...,  1.1560, -0.0194, -0.3539],\n",
       "         [ 0.4779, -0.5444, -0.0870,  ...,  1.4136, -0.5137, -0.3287],\n",
       "         ...,\n",
       "         [ 0.3512, -0.3564,  0.1628,  ..., -0.1383,  0.2157, -0.2248],\n",
       "         [ 0.2712, -0.2414, -0.2314,  ...,  0.8188, -0.1898, -0.2099],\n",
       "         [ 0.4083, -0.4160, -0.3753,  ...,  1.6491,  0.1184, -0.7137]],\n",
       "\n",
       "        [[-0.2620, -0.0454, -0.3714,  ..., -0.1002,  0.0469, -0.2382],\n",
       "         [-0.5508, -0.0658, -0.1082,  ...,  0.2486,  0.2475, -0.8336],\n",
       "         [-0.2339, -0.2428, -0.3974,  ..., -0.3725,  0.4164, -0.6616],\n",
       "         ...,\n",
       "         [-0.1728, -0.0819, -0.2361,  ..., -0.0804,  0.0541, -0.0938],\n",
       "         [-0.2688, -0.1920, -0.2137,  ..., -0.2153,  0.0508, -0.0768],\n",
       "         [-0.2743, -0.1995, -0.0615,  ..., -0.1269,  0.1729,  0.0613]],\n",
       "\n",
       "        [[-0.4732,  0.2295, -0.3679,  ..., -0.6695,  0.4404, -0.6043],\n",
       "         [-0.1313,  0.5733,  0.1188,  ..., -1.0586,  0.7702, -0.5091],\n",
       "         [-0.3901,  0.4017, -0.1077,  ..., -0.5972,  0.4294, -0.3685],\n",
       "         ...,\n",
       "         [-0.2051,  0.0500,  0.2165,  ..., -0.4726,  0.2502, -0.2102],\n",
       "         [-0.1932,  0.1971,  0.0154,  ..., -0.4866,  0.3652, -0.3435],\n",
       "         [-0.3149,  0.0539, -0.0269,  ..., -0.3351,  0.4055, -0.3261]],\n",
       "\n",
       "        [[ 0.3110, -0.7268, -0.2269,  ...,  0.3474, -0.1304, -0.5594],\n",
       "         [ 0.0655,  0.2444, -0.1067,  ...,  0.4197, -0.0750, -0.6878],\n",
       "         [-0.0034, -0.5242, -0.0098,  ...,  0.3549, -0.0285, -0.2405],\n",
       "         ...,\n",
       "         [ 0.1632, -0.5357, -0.0183,  ...,  0.1066, -0.0659, -0.3186],\n",
       "         [ 0.2368, -0.4840, -0.2428,  ...,  0.0998, -0.0954, -0.4190],\n",
       "         [ 0.1381, -0.5024, -0.2174,  ...,  0.1099, -0.0406, -0.4516]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.5931,  0.4240,  0.7164,  ...,  0.5025,  0.5345, -0.5526],\n",
       "        [-0.1618, -0.1864,  0.2477,  ...,  0.0472, -0.1520, -0.4127],\n",
       "        [-0.1059, -0.0272, -0.4032,  ..., -0.4657, -0.3133, -0.0110],\n",
       "        [-0.5632,  0.3321,  0.4845,  ...,  0.2915,  0.2838, -0.3766]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La carga con AutoModel perdio el head de Sequence Classification por alguna razon\n",
    "pt_outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1d6d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "428e09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs3 = model3(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ba5d90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs3.logits == pt_outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18b26cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = model3(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
    "all_hidden_states  = pt_outputs.hidden_states\n",
    "all_attentions = pt_outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "500c7489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f70dcd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs.logits ==  pt_outputs3.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b08c1769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs3.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b60c910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pt_outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a6c2df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d18a16",
   "metadata": {},
   "source": [
    "# Mas alla de AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5683f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3caf5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\", \n",
    "     \"El hotel era horrendo.\",\n",
    "     \"O hotel era esplÃªndido.\"\n",
    "    ],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "86f2ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6297d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.0833,  4.3364],\n",
       "        [ 0.0818, -0.0418],\n",
       "        [-2.2111,  2.2326],\n",
       "        [-1.8396,  1.8446]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f2232e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12810de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
    "     \"We hope you don't hate it.\", \n",
    "     \"I hate you\",\n",
    "     \"I love love.\"\n",
    "    ],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "437ba568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4469, 0.5531],\n",
       "        [0.4633, 0.5367],\n",
       "        [0.4452, 0.5548],\n",
       "        [0.4687, 0.5313]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(**batch).logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4419a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6413b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 1.   ],\n",
       "       [0.531, 0.469],\n",
       "       [0.999, 0.001],\n",
       "       [0.   , 1.   ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(**batch).logits, dim=-1).detach().numpy().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45490dcf",
   "metadata": {},
   "source": [
    "# [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html#fine-tuning-a-pretrained-model)\n",
    "\n",
    "* [datasets repo](https://github.com/huggingface/datasets)\n",
    "* [Datasets doc](https://huggingface.co/docs/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e162e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06394afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43824de1512e4f5c9bacf64fe6da79fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033d00c5eb494798bf609857d0e0df93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8f1027ed624e89bf6da22070d2ab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "42ccb2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus',\n",
       " 'ag_news',\n",
       " 'ai2_arc',\n",
       " 'air_dialogue',\n",
       " 'ajgt_twitter_ar',\n",
       " 'allegro_reviews',\n",
       " 'allocine',\n",
       " 'alt',\n",
       " 'amazon_polarity',\n",
       " 'amazon_reviews_multi',\n",
       " 'amazon_us_reviews',\n",
       " 'ambig_qa',\n",
       " 'amttl',\n",
       " 'anli',\n",
       " 'app_reviews',\n",
       " 'aqua_rat',\n",
       " 'aquamuse',\n",
       " 'ar_cov19',\n",
       " 'ar_res_reviews',\n",
       " 'ar_sarcasm',\n",
       " 'arabic_billion_words',\n",
       " 'arabic_pos_dialect',\n",
       " 'arabic_speech_corpus',\n",
       " 'arcd',\n",
       " 'arsentd_lev',\n",
       " 'art',\n",
       " 'arxiv_dataset',\n",
       " 'ascent_kb',\n",
       " 'aslg_pc12',\n",
       " 'asnq',\n",
       " 'asset',\n",
       " 'assin',\n",
       " 'assin2',\n",
       " 'atomic',\n",
       " 'autshumato',\n",
       " 'babi_qa',\n",
       " 'banking77',\n",
       " 'bbaw_egyptian',\n",
       " 'bbc_hindi_nli',\n",
       " 'bc2gm_corpus',\n",
       " 'best2009',\n",
       " 'bianet',\n",
       " 'bible_para',\n",
       " 'big_patent',\n",
       " 'billsum',\n",
       " 'bing_coronavirus_query_set',\n",
       " 'biomrc',\n",
       " 'blended_skill_talk',\n",
       " 'blimp',\n",
       " 'blog_authorship_corpus',\n",
       " 'bn_hate_speech',\n",
       " 'bookcorpus',\n",
       " 'bookcorpusopen',\n",
       " 'boolq',\n",
       " 'bprec',\n",
       " 'break_data',\n",
       " 'brwac',\n",
       " 'bsd_ja_en',\n",
       " 'bswac',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'cail2018',\n",
       " 'caner',\n",
       " 'capes',\n",
       " 'catalonia_independence',\n",
       " 'cawac',\n",
       " 'cbt',\n",
       " 'cc100',\n",
       " 'cc_news',\n",
       " 'ccaligned_multilingual',\n",
       " 'cdsc',\n",
       " 'cdt',\n",
       " 'cfq',\n",
       " 'chr_en',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'circa',\n",
       " 'civil_comments',\n",
       " 'clickbait_news_bg',\n",
       " 'climate_fever',\n",
       " 'clinc_oos',\n",
       " 'clue',\n",
       " 'cmrc2018',\n",
       " 'cnn_dailymail',\n",
       " 'coached_conv_pref',\n",
       " 'coarse_discourse',\n",
       " 'codah',\n",
       " 'code_search_net',\n",
       " 'code_x_glue_cc_clone_detection_big_clone_bench',\n",
       " 'code_x_glue_cc_clone_detection_poj104',\n",
       " 'code_x_glue_cc_cloze_testing_all',\n",
       " 'code_x_glue_cc_cloze_testing_maxmin',\n",
       " 'code_x_glue_cc_code_completion_line',\n",
       " 'code_x_glue_cc_code_completion_token',\n",
       " 'code_x_glue_cc_code_refinement',\n",
       " 'code_x_glue_cc_code_to_code_trans',\n",
       " 'code_x_glue_cc_defect_detection',\n",
       " 'code_x_glue_ct_code_to_text',\n",
       " 'code_x_glue_tc_nl_code_search_adv',\n",
       " 'code_x_glue_tc_text_to_code',\n",
       " 'code_x_glue_tt_text_to_text',\n",
       " 'com_qa',\n",
       " 'common_gen',\n",
       " 'common_voice',\n",
       " 'commonsense_qa',\n",
       " 'compguesswhat',\n",
       " 'conceptnet5',\n",
       " 'conll2000',\n",
       " 'conll2002',\n",
       " 'conll2003',\n",
       " 'conllpp',\n",
       " 'conv_ai',\n",
       " 'conv_ai_2',\n",
       " 'conv_ai_3',\n",
       " 'conv_questions',\n",
       " 'coqa',\n",
       " 'cord19',\n",
       " 'cornell_movie_dialog',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'counter',\n",
       " 'covid_qa_castorini',\n",
       " 'covid_qa_deepset',\n",
       " 'covid_qa_ucsd',\n",
       " 'covid_tweets_japanese',\n",
       " 'covost2',\n",
       " 'craigslist_bargains',\n",
       " 'crawl_domain',\n",
       " 'crd3',\n",
       " 'crime_and_punish',\n",
       " 'crows_pairs',\n",
       " 'cryptonite',\n",
       " 'cs_restaurants',\n",
       " 'cuad',\n",
       " 'curiosity_dialogs',\n",
       " 'daily_dialog',\n",
       " 'dane',\n",
       " 'danish_political_comments',\n",
       " 'dart',\n",
       " 'datacommons_factcheck',\n",
       " 'dbpedia_14',\n",
       " 'dbrd',\n",
       " 'deal_or_no_dialog',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dengue_filipino',\n",
       " 'dialog_re',\n",
       " 'diplomacy_detection',\n",
       " 'disaster_response_messages',\n",
       " 'discofuse',\n",
       " 'discovery',\n",
       " 'doc2dial',\n",
       " 'docred',\n",
       " 'doqa',\n",
       " 'dream',\n",
       " 'drop',\n",
       " 'duorc',\n",
       " 'dutch_social',\n",
       " 'dyk',\n",
       " 'e2e_nlg',\n",
       " 'e2e_nlg_cleaned',\n",
       " 'ecb',\n",
       " 'ecthr_cases',\n",
       " 'ehealth_kd',\n",
       " 'eitb_parcc',\n",
       " 'eli5',\n",
       " 'emea',\n",
       " 'emo',\n",
       " 'emotion',\n",
       " 'emotone_ar',\n",
       " 'empathetic_dialogues',\n",
       " 'enriched_web_nlg',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eth_py150_open',\n",
       " 'ethos',\n",
       " 'eu_regulatory_ir',\n",
       " 'eurlex',\n",
       " 'euronews',\n",
       " 'europa_eac_tm',\n",
       " 'europa_ecdc_tm',\n",
       " 'europarl_bilingual',\n",
       " 'event2Mind',\n",
       " 'evidence_infer_treatment',\n",
       " 'exams',\n",
       " 'factckbr',\n",
       " 'fake_news_english',\n",
       " 'fake_news_filipino',\n",
       " 'farsi_news',\n",
       " 'fashion_mnist',\n",
       " 'fever',\n",
       " 'few_rel',\n",
       " 'financial_phrasebank',\n",
       " 'finer',\n",
       " 'flores',\n",
       " 'flue',\n",
       " 'fquad',\n",
       " 'freebase_qa',\n",
       " 'gap',\n",
       " 'gem',\n",
       " 'generated_reviews_enth',\n",
       " 'generics_kb',\n",
       " 'german_legal_entity_recognition',\n",
       " 'germaner',\n",
       " 'germeval_14',\n",
       " 'giga_fren',\n",
       " 'gigaword',\n",
       " 'glucose',\n",
       " 'glue',\n",
       " 'gnad10',\n",
       " 'go_emotions',\n",
       " 'gooaq',\n",
       " 'google_wellformed_query',\n",
       " 'grail_qa',\n",
       " 'great_code',\n",
       " 'guardian_authorship',\n",
       " 'gutenberg_time',\n",
       " 'hans',\n",
       " 'hansards',\n",
       " 'hard',\n",
       " 'harem',\n",
       " 'has_part',\n",
       " 'hate_offensive',\n",
       " 'hate_speech18',\n",
       " 'hate_speech_filipino',\n",
       " 'hate_speech_offensive',\n",
       " 'hate_speech_pl',\n",
       " 'hate_speech_portuguese',\n",
       " 'hatexplain',\n",
       " 'hausa_voa_ner',\n",
       " 'hausa_voa_topics',\n",
       " 'hda_nli_hindi',\n",
       " 'head_qa',\n",
       " 'health_fact',\n",
       " 'hebrew_projectbenyehuda',\n",
       " 'hebrew_sentiment',\n",
       " 'hebrew_this_world',\n",
       " 'hellaswag',\n",
       " 'hendrycks_test',\n",
       " 'hind_encorp',\n",
       " 'hindi_discourse',\n",
       " 'hippocorpus',\n",
       " 'hkcancor',\n",
       " 'hlgd',\n",
       " 'hope_edi',\n",
       " 'hotpot_qa',\n",
       " 'hover',\n",
       " 'hrenwac_para',\n",
       " 'hrwac',\n",
       " 'humicroedit',\n",
       " 'hybrid_qa',\n",
       " 'hyperpartisan_news_detection',\n",
       " 'iapp_wiki_qa_squad',\n",
       " 'id_clickbait',\n",
       " 'id_liputan6',\n",
       " 'id_nergrit_corpus',\n",
       " 'id_newspapers_2018',\n",
       " 'id_panl_bppt',\n",
       " 'id_puisi',\n",
       " 'igbo_english_machine_translation',\n",
       " 'igbo_monolingual',\n",
       " 'igbo_ner',\n",
       " 'ilist',\n",
       " 'imdb',\n",
       " 'imdb_urdu_reviews',\n",
       " 'imppres',\n",
       " 'indic_glue',\n",
       " 'indonlu',\n",
       " 'inquisitive_qg',\n",
       " 'interpress_news_category_tr',\n",
       " 'interpress_news_category_tr_lite',\n",
       " 'irc_disentangle',\n",
       " 'isixhosa_ner_corpus',\n",
       " 'isizulu_ner_corpus',\n",
       " 'iwslt2017',\n",
       " 'jeopardy',\n",
       " 'jfleg',\n",
       " 'jigsaw_toxicity_pred',\n",
       " 'jnlpba',\n",
       " 'journalists_questions',\n",
       " 'kannada_news',\n",
       " 'kd_conv',\n",
       " 'kde4',\n",
       " 'kelm',\n",
       " 'kilt_tasks',\n",
       " 'kilt_wikipedia',\n",
       " 'kinnews_kirnews',\n",
       " 'klue',\n",
       " 'kor_3i4k',\n",
       " 'kor_hate',\n",
       " 'kor_ner',\n",
       " 'kor_nli',\n",
       " 'kor_nlu',\n",
       " 'kor_qpair',\n",
       " 'kor_sae',\n",
       " 'kor_sarcasm',\n",
       " 'labr',\n",
       " 'lama',\n",
       " 'lambada',\n",
       " 'large_spanish_corpus',\n",
       " 'laroseda',\n",
       " 'lc_quad',\n",
       " 'lener_br',\n",
       " 'liar',\n",
       " 'librispeech_asr',\n",
       " 'librispeech_lm',\n",
       " 'limit',\n",
       " 'lince',\n",
       " 'linnaeus',\n",
       " 'liveqa',\n",
       " 'lj_speech',\n",
       " 'lm1b',\n",
       " 'lst20',\n",
       " 'm_lama',\n",
       " 'mac_morpho',\n",
       " 'makhzan',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'matinf',\n",
       " 'mc_taco',\n",
       " 'md_gender_bias',\n",
       " 'mdd',\n",
       " 'med_hop',\n",
       " 'medal',\n",
       " 'medical_dialog',\n",
       " 'medical_questions_pairs',\n",
       " 'menyo20k_mt',\n",
       " 'meta_woz',\n",
       " 'metooma',\n",
       " 'metrec',\n",
       " 'miam',\n",
       " 'mkb',\n",
       " 'mkqa',\n",
       " 'mlqa',\n",
       " 'mlsum',\n",
       " 'mnist',\n",
       " 'mocha',\n",
       " 'moroco',\n",
       " 'movie_rationales',\n",
       " 'mrqa',\n",
       " 'ms_marco',\n",
       " 'ms_terms',\n",
       " 'msr_genomics_kbcomp',\n",
       " 'msr_sqa',\n",
       " 'msr_text_compression',\n",
       " 'msr_zhen_translation_parity',\n",
       " 'msra_ner',\n",
       " 'mt_eng_vietnamese',\n",
       " 'muchocine',\n",
       " 'multi_booked',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'multi_para_crawl',\n",
       " 'multi_re_qa',\n",
       " 'multi_woz_v22',\n",
       " 'multi_x_science_sum',\n",
       " 'mutual_friends',\n",
       " 'mwsc',\n",
       " 'myanmar_news',\n",
       " 'narrativeqa',\n",
       " 'narrativeqa_manual',\n",
       " 'natural_questions',\n",
       " 'ncbi_disease',\n",
       " 'nchlt',\n",
       " 'ncslgr',\n",
       " 'nell',\n",
       " 'neural_code_search',\n",
       " 'news_commentary',\n",
       " 'newsgroup',\n",
       " 'newsph',\n",
       " 'newsph_nli',\n",
       " 'newspop',\n",
       " 'newsqa',\n",
       " 'newsroom',\n",
       " 'nkjp-ner',\n",
       " 'nli_tr',\n",
       " 'nlu_evaluation_data',\n",
       " 'norec',\n",
       " 'norne',\n",
       " 'norwegian_ner',\n",
       " 'nq_open',\n",
       " 'nsmc',\n",
       " 'numer_sense',\n",
       " 'numeric_fused_head',\n",
       " 'oclar',\n",
       " 'offcombr',\n",
       " 'offenseval2020_tr',\n",
       " 'offenseval_dravidian',\n",
       " 'ofis_publik',\n",
       " 'ohsumed',\n",
       " 'ollie',\n",
       " 'omp',\n",
       " 'onestop_english',\n",
       " 'open_subtitles',\n",
       " 'openbookqa',\n",
       " 'openslr',\n",
       " 'openwebtext',\n",
       " 'opinosis',\n",
       " 'opus100',\n",
       " 'opus_books',\n",
       " 'opus_dgt',\n",
       " 'opus_dogc',\n",
       " 'opus_elhuyar',\n",
       " 'opus_euconst',\n",
       " 'opus_finlex',\n",
       " 'opus_fiskmo',\n",
       " 'opus_gnome',\n",
       " 'opus_infopankki',\n",
       " 'opus_memat',\n",
       " 'opus_montenegrinsubs',\n",
       " 'opus_openoffice',\n",
       " 'opus_paracrawl',\n",
       " 'opus_rf',\n",
       " 'opus_tedtalks',\n",
       " 'opus_ubuntu',\n",
       " 'opus_wikipedia',\n",
       " 'opus_xhosanavy',\n",
       " 'orange_sum',\n",
       " 'oscar',\n",
       " 'para_crawl',\n",
       " 'para_pat',\n",
       " 'parsinlu_reading_comprehension',\n",
       " 'paws',\n",
       " 'paws-x',\n",
       " 'pec',\n",
       " 'peer_read',\n",
       " 'peoples_daily_ner',\n",
       " 'per_sent',\n",
       " 'persian_ner',\n",
       " 'pg19',\n",
       " 'php',\n",
       " 'piaf',\n",
       " 'pib',\n",
       " 'piqa',\n",
       " 'pn_summary',\n",
       " 'poem_sentiment',\n",
       " 'polemo2',\n",
       " 'poleval2019_cyberbullying',\n",
       " 'poleval2019_mt',\n",
       " 'polsum',\n",
       " 'polyglot_ner',\n",
       " 'prachathai67k',\n",
       " 'pragmeval',\n",
       " 'proto_qa',\n",
       " 'psc',\n",
       " 'ptb_text_only',\n",
       " 'pubmed',\n",
       " 'pubmed_qa',\n",
       " 'py_ast',\n",
       " 'qa4mre',\n",
       " 'qa_srl',\n",
       " 'qa_zre',\n",
       " 'qangaroo',\n",
       " 'qanta',\n",
       " 'qasc',\n",
       " 'qasper',\n",
       " 'qed',\n",
       " 'qed_amara',\n",
       " 'quac',\n",
       " 'quail',\n",
       " 'quarel',\n",
       " 'quartz',\n",
       " 'quora',\n",
       " 'quoref',\n",
       " 'race',\n",
       " 're_dial',\n",
       " 'reasoning_bg',\n",
       " 'recipe_nlg',\n",
       " 'reclor',\n",
       " 'reddit',\n",
       " 'reddit_tifu',\n",
       " 'refresd',\n",
       " 'reuters21578',\n",
       " 'ro_sent',\n",
       " 'ro_sts',\n",
       " 'ro_sts_parallel',\n",
       " 'roman_urdu',\n",
       " 'ronec',\n",
       " 'ropes',\n",
       " 'rotten_tomatoes',\n",
       " 's2orc',\n",
       " 'samsum',\n",
       " 'sanskrit_classic',\n",
       " 'saudinewsnet',\n",
       " 'scan',\n",
       " 'scb_mt_enth_2020',\n",
       " 'schema_guided_dstc8',\n",
       " 'scicite',\n",
       " 'scielo',\n",
       " 'scientific_papers',\n",
       " 'scifact',\n",
       " 'sciq',\n",
       " 'scitail',\n",
       " 'scitldr',\n",
       " 'search_qa',\n",
       " 'selqa',\n",
       " 'sem_eval_2010_task_8',\n",
       " 'sem_eval_2014_task_1',\n",
       " 'sem_eval_2020_task_11',\n",
       " 'sent_comp',\n",
       " 'senti_lex',\n",
       " 'senti_ws',\n",
       " 'sentiment140',\n",
       " 'sepedi_ner',\n",
       " 'sesotho_ner_corpus',\n",
       " 'setimes',\n",
       " 'setswana_ner_corpus',\n",
       " 'sharc',\n",
       " 'sharc_modified',\n",
       " 'sick',\n",
       " 'silicone',\n",
       " 'simple_questions_v2',\n",
       " 'siswati_ner_corpus',\n",
       " 'smartdata',\n",
       " 'sms_spam',\n",
       " 'snips_built_in_intents',\n",
       " 'snli',\n",
       " 'snow_simplified_japanese_corpus',\n",
       " 'so_stacksample',\n",
       " 'social_bias_frames',\n",
       " 'social_i_qa',\n",
       " 'sofc_materials_articles',\n",
       " 'sogou_news',\n",
       " 'spanish_billion_words',\n",
       " 'spc',\n",
       " 'species_800',\n",
       " 'spider',\n",
       " 'squad',\n",
       " 'squad_adversarial',\n",
       " 'squad_es',\n",
       " 'squad_it',\n",
       " 'squad_kor_v1',\n",
       " 'squad_kor_v2',\n",
       " 'squad_v1_pt',\n",
       " 'squad_v2',\n",
       " 'squadshifts',\n",
       " 'srwac',\n",
       " 'sst',\n",
       " 'stereoset',\n",
       " 'stsb_mt_sv',\n",
       " 'stsb_multi_mt',\n",
       " 'style_change_detection',\n",
       " 'subjqa',\n",
       " 'super_glue',\n",
       " 'swag',\n",
       " 'swahili',\n",
       " 'swahili_news',\n",
       " 'swda',\n",
       " 'swedish_ner_corpus',\n",
       " 'swedish_reviews',\n",
       " 'tab_fact',\n",
       " 'tamilmixsentiment',\n",
       " 'tanzil',\n",
       " 'tapaco',\n",
       " 'tashkeela',\n",
       " 'taskmaster1',\n",
       " 'taskmaster2',\n",
       " 'taskmaster3',\n",
       " 'tatoeba',\n",
       " 'ted_hrlr',\n",
       " 'ted_iwlst2013',\n",
       " 'ted_multi',\n",
       " 'ted_talks_iwslt',\n",
       " 'telugu_books',\n",
       " 'telugu_news',\n",
       " 'tep_en_fa_para',\n",
       " 'thai_toxicity_tweet',\n",
       " 'thainer',\n",
       " 'thaiqa_squad',\n",
       " 'thaisum',\n",
       " 'tilde_model',\n",
       " 'times_of_india_news_headlines',\n",
       " 'timit_asr',\n",
       " 'tiny_shakespeare',\n",
       " 'tlc',\n",
       " 'tmu_gfm_dataset',\n",
       " 'totto',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tsac',\n",
       " 'ttc4900',\n",
       " 'tunizi',\n",
       " 'tuple_ie',\n",
       " 'turk',\n",
       " 'turkish_movie_sentiment',\n",
       " 'turkish_ner',\n",
       " 'turkish_product_reviews',\n",
       " 'turkish_shrinked_ner',\n",
       " 'turku_ner_corpus',\n",
       " 'tweet_eval',\n",
       " 'tweet_qa',\n",
       " 'tweets_ar_en_parallel',\n",
       " 'tweets_hate_speech_detection',\n",
       " 'twi_text_c3',\n",
       " 'twi_wordsim353',\n",
       " 'tydiqa',\n",
       " 'ubuntu_dialogs_corpus',\n",
       " 'udhr',\n",
       " 'um005',\n",
       " 'un_ga',\n",
       " 'un_multi',\n",
       " 'un_pc',\n",
       " 'universal_dependencies',\n",
       " 'universal_morphologies',\n",
       " 'urdu_fake_news',\n",
       " 'urdu_sentiment_corpus',\n",
       " 'web_nlg',\n",
       " 'web_of_science',\n",
       " 'web_questions',\n",
       " 'weibo_ner',\n",
       " 'wi_locness',\n",
       " 'wiki40b',\n",
       " 'wiki_asp',\n",
       " 'wiki_atomic_edits',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dpr',\n",
       " 'wiki_hop',\n",
       " 'wiki_lingua',\n",
       " 'wiki_movies',\n",
       " 'wiki_qa',\n",
       " 'wiki_qa_ar',\n",
       " 'wiki_snippets',\n",
       " 'wiki_source',\n",
       " 'wiki_split',\n",
       " 'wiki_summary',\n",
       " 'wikiann',\n",
       " 'wikicorpus',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikisql',\n",
       " 'wikitext',\n",
       " 'wikitext_tl39',\n",
       " 'wili_2018',\n",
       " 'wino_bias',\n",
       " 'winograd_wsc',\n",
       " 'winogrande',\n",
       " 'wiqa',\n",
       " 'wisesight1000',\n",
       " 'wisesight_sentiment',\n",
       " 'wmt14',\n",
       " 'wmt15',\n",
       " 'wmt16',\n",
       " 'wmt17',\n",
       " 'wmt18',\n",
       " 'wmt19',\n",
       " 'wmt20_mlqe_task1',\n",
       " 'wmt20_mlqe_task2',\n",
       " 'wmt20_mlqe_task3',\n",
       " 'wmt_t2t',\n",
       " 'wnut_17',\n",
       " 'wongnai_reviews',\n",
       " 'woz_dialogue',\n",
       " 'wrbsc',\n",
       " 'x_stance',\n",
       " 'xcopa',\n",
       " 'xed_en_fi',\n",
       " 'xglue',\n",
       " 'xnli',\n",
       " 'xor_tydi_qa',\n",
       " 'xquad',\n",
       " 'xquad_r',\n",
       " 'xsum',\n",
       " 'xsum_factuality',\n",
       " 'xtreme',\n",
       " 'yahoo_answers_qa',\n",
       " 'yahoo_answers_topics',\n",
       " 'yelp_polarity',\n",
       " 'yelp_review_full',\n",
       " 'yoruba_bbc_topics',\n",
       " 'yoruba_gv_ner',\n",
       " 'yoruba_text_c3',\n",
       " 'yoruba_wordsim353',\n",
       " 'youtube_caption_corrections',\n",
       " 'zest',\n",
       " 'AConsApart/anime_subtitles_DialoGPT',\n",
       " 'Abdo1Kamr/Arabic_Nine_Hadiths_Books',\n",
       " 'AdWeeb/DravidianMT',\n",
       " 'Adnan/Urdu_News_Headlines',\n",
       " 'Akshith/aa',\n",
       " 'Akshith/g_rock',\n",
       " 'Akshith/test',\n",
       " 'Annielytics/DoctorsNotes',\n",
       " 'Avishekavi/Avi',\n",
       " 'Binbin/my_dataset',\n",
       " 'Darren/data',\n",
       " 'EMBO/biolang',\n",
       " 'EMBO/sd-nlp',\n",
       " 'Eymen3455/xsum_tr',\n",
       " 'FRTNX/cosuju',\n",
       " 'Felix-ML/quoteli3',\n",
       " 'Firoj/CrisisBench',\n",
       " 'Fraser/mnist-text-default',\n",
       " 'Fraser/mnist-text-no-spaces',\n",
       " 'Fraser/mnist-text-small',\n",
       " 'Fraser/news-category-dataset',\n",
       " 'Fraser/python-lines',\n",
       " 'Fraser/short-jokes',\n",
       " 'Halilyesilceng/autonlp-data-nameEntityRecognition',\n",
       " 'HarleyQ/WitcherDialogue',\n",
       " 'Harveenchadha/Gujarati-Monolingual-Data',\n",
       " 'Jean-Baptiste/wikiner_fr',\n",
       " 'KETI-AIR/klue',\n",
       " 'KETI-AIR/kor_corpora',\n",
       " 'KETI-AIR/korquad',\n",
       " 'KETI-AIR/nikl',\n",
       " 'LIAMF-USP/arc-retrieval-c4',\n",
       " 'MKK/Dhivehi-English',\n",
       " 'MarianaSahagun/test',\n",
       " 'Melinoe/TheLabTexts',\n",
       " 'NTUYG/RAGTest',\n",
       " 'NbAiLab/norec_agg',\n",
       " 'NbAiLab/norne',\n",
       " 'NbAiLab/norwegian_parliament',\n",
       " 'Ofrit/tmp',\n",
       " 'QA/abk-eng',\n",
       " 'Remesita/tagged_reviews',\n",
       " 'SajjadAyoubi/persian_qa',\n",
       " 'TRoboto/masc',\n",
       " 'Tatyana/ru_sentiment_dataset',\n",
       " 'Terry0107/RiSAWOZ',\n",
       " 'TimTreasure4/Test',\n",
       " 'Trainmaster9977/957',\n",
       " 'Trainmaster9977/zbakuman',\n",
       " 'Tyler/wikimatrix_collapsed',\n",
       " 'Valahaar/wsdmt',\n",
       " 'Vishva/UniFAQ_DataSET',\n",
       " 'Wikidepia/IndoParaCrawl',\n",
       " 'Wikidepia/IndoSQuAD',\n",
       " 'XiangXiang/clt',\n",
       " 'Yves/fhnw_swiss_parliament',\n",
       " 'abhishek/autonlp-data-imdb_eval',\n",
       " 'abwicke/C-B-R',\n",
       " 'abwicke/koplo',\n",
       " 'adamlin/re_dial',\n",
       " 'ajmbell/test-dataset',\n",
       " 'alireza655/alireza655',\n",
       " 'allenai/c4',\n",
       " 'anukaver/EstQA',\n",
       " 'ashish-shrivastava/dont-know-dataset',\n",
       " 'astarostap/antisemitic-tweets',\n",
       " 'astarostap/antisemitic_tweets',\n",
       " 'athivvat/thai-rap-lyrics',\n",
       " 'ausgequetschtem/jtrddfhfgh',\n",
       " 'bavard/personachat_truecased',\n",
       " 'bemanningssitua/dplremjfjfj',\n",
       " 'berkergurcay/2020-10K-Reports',\n",
       " 'bsc/ancora-ca-ner',\n",
       " 'bsc/sts-ca',\n",
       " 'bsc/tecla',\n",
       " 'bsc/viquiquad',\n",
       " 'bsc/xquad-ca',\n",
       " 'caca/zscczs',\n",
       " 'canwenxu/dogwhistle',\n",
       " 'ccccccc/hdjw_94ejrjr',\n",
       " 'cdminix/mgb1',\n",
       " 'cemigo/taylor_vs_shakes',\n",
       " 'cemigo/test-data',\n",
       " 'cheulyop/ksponspeech',\n",
       " 'clarin-pl/cst-wikinews',\n",
       " 'clarin-pl/nkjp-pos',\n",
       " 'clarin-pl/polemo2-official',\n",
       " 'classla/copa_hr',\n",
       " 'classla/hr500k',\n",
       " 'classla/reldi_hr',\n",
       " 'classla/reldi_sr',\n",
       " 'classla/setimes_sr',\n",
       " 'cnrcastroli/aaaa',\n",
       " 'congpt/dstc23_asr',\n",
       " 'corypaik/prost',\n",
       " 'ctl/ConceptualCaptions',\n",
       " 'dasago78/dasago78dataset',\n",
       " 'dataset/wikipedia_bn',\n",
       " 'david-wb/zeshel',\n",
       " 'deepset/germandpr',\n",
       " 'deepset/germanquad',\n",
       " 'dfgvhxfgv/fghghj',\n",
       " 'dgknrsln/Yorumsepeti',\n",
       " 'dispenst/jhghdghfd',\n",
       " 'dispix/test-dataset',\n",
       " 'dynabench/dynasent',\n",
       " 'dynabench/qa',\n",
       " 'eason929/test',\n",
       " 'edfews/szdfcszdf',\n",
       " 'edsas/fgrdtgrdtdr',\n",
       " 'edsas/grttyi',\n",
       " 'ervis/aaa',\n",
       " 'ervis/qqq',\n",
       " 'fatvvs/autonlp-data-entity_model_conll2003',\n",
       " 'formermagic/github_python_1m',\n",
       " 'formu/CVT',\n",
       " 'fulai/DuReader',\n",
       " 'fuliucansheng/data_for_test',\n",
       " 'fvillena/cantemist',\n",
       " 'fvillena/spanish_diagnostics',\n",
       " 'german-nlp-group/german_common_crawl',\n",
       " 'godzillavskongonlinetv/ergfdg',\n",
       " 'godzillavskongonlinetv/godzillavskongfullmovie',\n",
       " 'gpt3mix/rt20',\n",
       " 'gpt3mix/sst2',\n",
       " 'gustavecortal/fr_covid_news',\n",
       " 'hartzeer/kdfjdshfje',\n",
       " 'hfface/poopi',\n",
       " 'huggingFaceUser02/air21_grp13_tokenized_results',\n",
       " 'huseinzol05/translated-The-Pile',\n",
       " 'iamshsdf/sssssssssss',\n",
       " 'jaimin/wav2vec2-large-xlsr-gujarati-demo',\n",
       " 'jdepoix/junit_test_completion',\n",
       " 'jglaser/binding_affinity',\n",
       " 'jimregan/clarinpl_sejmsenat',\n",
       " 'jimregan/clarinpl_studio',\n",
       " 'jmamou/augmented-glue-sst2',\n",
       " 'joelito/ler',\n",
       " 'joelito/sem_eval_2010_task_8',\n",
       " 'julien-c/dummy-dataset-from-colab',\n",
       " 'julien-c/reactiongif',\n",
       " 'k-halid/ar',\n",
       " 'karinev/lanuitdudroit',\n",
       " 'katoensp/VR-OP',\n",
       " 'kmyoo/klue-tc-dev',\n",
       " 'lavis-nlp/german_legal_sentences',\n",
       " 'lewtun/binary_classification_dummy',\n",
       " 'lewtun/text_classification_dummy',\n",
       " 'lhoestq/custom_squad',\n",
       " 'lhoestq/squad',\n",
       " 'lhoestq/test',\n",
       " 'lhoestq/wikipedia_bn',\n",
       " 'lkiouiou/o9ui7877687',\n",
       " 'lohanna/testedjkcxkf',\n",
       " 'lucien/sciencemission',\n",
       " 'lucien/voacantonesed',\n",
       " 'lucien/wsaderfffjjjhhh',\n",
       " 'lucio/common_voice_eval',\n",
       " 'majod/CleanNaturalQuestionsDataset',\n",
       " 'makanan/umich',\n",
       " 'medzaf/test',\n",
       " 'metalearning/kaggale-nlp-tutorial',\n",
       " 'mksaad/Arabic_news',\n",
       " 'mmm-da/rutracker_anime_torrent_titles',\n",
       " 'mohsenfayyaz/toxicity-classification-datasets',\n",
       " 'mrojas/abbreviation',\n",
       " 'mrojas/body',\n",
       " 'mrojas/disease',\n",
       " 'mrojas/family',\n",
       " 'mrojas/finding',\n",
       " 'mrojas/medication',\n",
       " 'mrojas/procedure',\n",
       " 'mulcyber/europarl-mono',\n",
       " 'mustafa12/db_ee',\n",
       " 'mustafa12/edaaaas',\n",
       " 'mustafa12/thors',\n",
       " 'nateraw/cats-and-dogs',\n",
       " 'nateraw/fairface',\n",
       " 'nateraw/test',\n",
       " 'naver-clova-conversation/klue-tc-dev-tsv',\n",
       " 'naver-clova-conversation/klue-tc-tsv',\n",
       " 'naver-clova-conversation-ul/klue-tc-dev',\n",
       " 'nbroad/few-nerd',\n",
       " 'nucklehead/ht-voice-dataset',\n",
       " 'oelkrise/CRT',\n",
       " 'osanseviero/llama_test',\n",
       " 'parivartanayurveda/Malesexproblemsayurvedictreatment',\n",
       " 'pasinit/xlwic',\n",
       " 'patrickvonplaten/librispeech_asr_dummy',\n",
       " 'patrickvonplaten/scientific_papers_dummy',\n",
       " 'pdesoyres/test',\n",
       " 'peixian/equity_evaluation_corpus',\n",
       " 'peixian/rtGender',\n",
       " 'pelican/test_100',\n",
       " 'persiannlp/parsinlu_entailment',\n",
       " 'persiannlp/parsinlu_query_paraphrasing',\n",
       " 'persiannlp/parsinlu_reading_comprehension',\n",
       " 'persiannlp/parsinlu_sentiment',\n",
       " 'persiannlp/parsinlu_translation_en_fa',\n",
       " 'persiannlp/parsinlu_translation_fa_en',\n",
       " 'piEsposito/br-quad-2.0',\n",
       " 'piEsposito/br_quad_20',\n",
       " 'piEsposito/squad_20_ptbr',\n",
       " 'princeton-nlp/datasets-for-simcse',\n",
       " 'priya3301/Graduation_admission',\n",
       " 'priya3301/tes',\n",
       " 'priya3301/test',\n",
       " 'rewardsignal/reddit_writing_prompts',\n",
       " 'rony/soccer-dialogues',\n",
       " 'roskoN/dstc8-reddit-corpus',\n",
       " 'salesken/Paraphrase_category_detection',\n",
       " 'sdfufygvjh/fgghuviugviu',\n",
       " 'seamew/Weibo',\n",
       " 'seamew/amazon_reviews_zh',\n",
       " 'seamew/weibo_avg',\n",
       " 'shahrukhx01/questions-vs-statements',\n",
       " 'sharejing/BiPaR',\n",
       " 'sileod/metaeval',\n",
       " 'sismetanin/rureviews',\n",
       " 'smallv0221/my-test',\n",
       " 'somaimanguyat/Salome',\n",
       " 'somaimanguyat/movie21',\n",
       " 'somaimanguyat/xiomay',\n",
       " 'spacemanidol/ms_marco_doc2query',\n",
       " 'spacemanidol/msmarco_passage_ranking',\n",
       " 'ssasaa/gghghgh',\n",
       " 'sshleifer/pseudo_bart_xsum',\n",
       " 'stas/openwebtext-10k',\n",
       " 'stas/wmt14-en-de-pre-processed',\n",
       " 'stas/wmt16-en-ro-pre-processed',\n",
       " 'stiel/skjdhjkasdhasjkd',\n",
       " 'subiksha/OwnDataset',\n",
       " 'susumu2357/squad_v2_sv',\n",
       " 'tals/test',\n",
       " 'tarudesu/UIT-ViCTSD',\n",
       " 'thiemowa/argumentationreviewcorpus',\n",
       " 'thiemowa/empathyreviewcorpus',\n",
       " 'tommy19970714/common_voice',\n",
       " 'tommy19970714/jsut_asr',\n",
       " 'tommy19970714/jsut_asr_hiragana',\n",
       " 'tommy19970714/jsut_asr_hiragana_small',\n",
       " 'tommy19970714/laborotvspeech',\n",
       " 'turingbench/TuringBench',\n",
       " 'uasoyasser/rgfes',\n",
       " 'vasudevgupta/bigbird-tokenized-natural-questions',\n",
       " 'vasudevgupta/data',\n",
       " 'vasudevgupta/natural-questions-validation',\n",
       " 'vasudevgupta/temperature-distribution-2d-plate',\n",
       " 'vasudevgupta/temperature-distribution-3d-cylinder',\n",
       " 'vctc92/sdsd',\n",
       " 'vctc92/test',\n",
       " 'versae/adobo',\n",
       " 'vershasaxena91/datasets',\n",
       " 'vershasaxena91/squad_multitask',\n",
       " 'w-nicole/childes_data',\n",
       " 'w11wo/imdb-javanese',\n",
       " 'webek18735/ddvoacantonesed',\n",
       " 'webek18735/dhikhscook',\n",
       " 'wmt/europarl',\n",
       " 'wmt/news-commentary',\n",
       " 'wmt/uncorpus',\n",
       " 'wmt/wikititles',\n",
       " 'wmt/wmt10',\n",
       " 'wmt/wmt13',\n",
       " 'wmt/wmt14',\n",
       " 'wmt/wmt15',\n",
       " 'wmt/wmt16',\n",
       " 'wmt/wmt17',\n",
       " 'wmt/wmt18',\n",
       " 'wmt/wmt19',\n",
       " 'yluisfern/PBU']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a695e730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1d3ec409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6098aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fe5c2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1005f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.&lt;br /&gt;&lt;br /&gt;But what if yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.&lt;br /&gt;&lt;br /&gt;Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes its an art... to successfully make a slow paced thriller.&lt;br /&gt;&lt;br /&gt;The story unfolds in nice volumes while you don't even notice it happening.&lt;br /&gt;&lt;br /&gt;Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>In this \"critically acclaimed psychological thriller based on true events, Gabriel (Robin Williams), a celebrated writer and late-night talk show host, becomes captivated by the harrowing story of a young listener and his adoptive mother (Toni Collette). When troubling questions arise about this boy's (story), however, Gabriel finds himself drawn into a widening mystery that hides a deadly secretÂ…",
       "\" according to film's official synopsis.&lt;br /&gt;&lt;br /&gt;You really should STOP reading these comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>THE NIGHT LISTENER (2006) **1/2 Robin Williams, Toni Collette, Bobby Cannavale, Rory Culkin, Joe Morton, Sandra Oh, John Cullum, Lisa Emery, Becky Ann Baker. (Dir: Patrick Stettner) &lt;br /&gt;&lt;br /&gt;Hitchcockian suspenser gives Williams a stand-out low-key performance.&lt;br /&gt;&lt;br /&gt;What is it about celebrities and fans? What is the near paranoia one associates with the other and why is it almost the norm? &lt;br /&gt;&lt;br /&gt;In the latest derange fan scenario, based on true events no less, Williams stars a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>You know, Robin Williams, God bless him, is constantly shooting himself in the foot lately with all these dumb comedies he has done this decade (with perhaps the exception of \"Death To Smoochy\", which bombed when it came out but is now a cult classic). The dramas he has made lately have been fantastic, especially \"Insomnia\" and \"One Hour Photo\". \"The Night Listener\", despite mediocre reviews and a quick DVD release, is among his best work, period.&lt;br /&gt;&lt;br /&gt;This is a very chilling story, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...&lt;br /&gt;&lt;br /&gt;Writer Armistead Maupin and director Patrick Stettner have truly succeeded! &lt;br /&gt;&lt;br /&gt;With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the film. Some of the action scenes were very interesting, tense and well done. I especially liked the opening scene which had a semi truck in it. A very tense action scene that seemed well done.&lt;br /&gt;&lt;br /&gt;Some of the transitional scenes were filmed in interesting ways such as time lapse photography, unusual colors, or interesting angles. Also the film is funny is several parts. I also liked how the evil guy was portrayed too. I'd give the film an 8 out of 10.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>There are many illnesses born in the mind of man which have been given life in modern times. Constant vigilance or accrued information in the realm of Pyschosis, have kept psychologists, counselors and psychiatrists busy with enough work to last them decades. Occasionally, some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence. That is the premise of the film entitled \" The Night Listner.\" It tells the story of a popular radio host ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>I enjoyed The Night Listener very much. It's one of the better movies of the summer.&lt;br /&gt;&lt;br /&gt;Robin Williams gives one of his best performances. In fact, the entire cast was very good. All played just the right notes for their characters - not too much and not too little. Sandra Oh adds a wonderful comic touch. Toni Collette is great as the Mom, and never goes over the top. Everyone is very believable.&lt;br /&gt;&lt;br /&gt;It's a short movie, just under an hour and a half. I noticed the general rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>The Night Listener is probably not one of William's best roles, but he makes a very interesting character in a somewhat odd but very different movie. I can guarantee you that you have never seen this kind of movie before. Some people maybe won't like the slow pacing of this movie, but I think it's the great plus of the movie. It is definitely one of the top movies that have come out the year 2006. It has a intriguing performance in a movie with a great content, dramatic feeling. This is no a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Like one of the previous commenters said, this had the foundations of a great movie but something happened on the way to delivery. Such a waste because Collette's performance was eerie and Williams was believable. I just kept waiting for it to get better. I don't think it was bad editing or needed another director, it could have just been the film. It came across as a Canadian movie, something like the first few seasons of X-Files. Not cheap, just hokey. Also, it needed a little more suspens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>The Night Listener held my attention, with Robin Williams shining as a New York City radio host who becomes enamored with his friendship with a 14 year old boy (Rory Culkin) who is very ill. Williams has never met the boy in person, as they have only been in contact by talking on the telephone. However, Williams' ex-boyfriend (nice job from Bobby Cannavale) raises doubt about the boy, which prompts Williams to arrange a meeting with him in person. What follows makes a permanent impact on Wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. &lt;br /&gt;&lt;br /&gt;He also acquaints himself with Pete's guardian,a woman named Donn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>If there is one thing to recommend about this film is that it is intriguing. The premise certainly draws the audience in because it is a mystery, and throughout the film there are hints that there is something dark lurking about. However, there is not much tension, and Williams' mild mannered portrayal doesn't do much to makes us relate to his obsession with the boy.&lt;br /&gt;&lt;br /&gt;Collete fares much better as the woman whose true nature and intentions are not very clear. The production felt rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>I absolutely LOVED this film! I do not at all relate to all the other comments I have read about it. I was COMPLETELY enthralled through every second! &lt;br /&gt;&lt;br /&gt;I found the story gripping, the acting intense, and the direction spot-on. I would literally jump every time the phone would ring close to the end of the movie. Even though there was nothing \"scary\" about the story itself, I was soundly on edge through the whole movie - and for the rest of my evening. &lt;br /&gt;&lt;br /&gt;I found that there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>This Night Listener is better than people are generally saying. It has weaknesses, and it seems to be having a genre identity crisis, no doubt, but I think its creepy atmosphere and intriguing performances make up for this. The whole thing feels like one of those fireside \"this happened to a friend of a friend of mine\" ghost stories. One big complaint about the movie is the pacing: but the slow and sometimes awkward pacing is deliberate. Everything that unfolds in this movie is kept well wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>When it comes to movies I can be pretty picky, and I'll complain about anything and everything that is done wrong. While every movie has its flaws, The Night Listener had an exceptionally low count.&lt;br /&gt;&lt;br /&gt;If you read the last review (it was hard, since half of it was written in caps and it contained no actual information about the movie), you may have been led to believe that this movie was not too well done. Unfortunately, if you read more than 3 lines into that same review, you discov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>Somewhat funny and well-paced action thriller that has Jamie Foxx as a hapless, fast-talking hoodlum who is chosen by an overly demanding U.S. Treasury Agent (David Morse) to be released on the streets of New York to find a picky computer thief/hacker (Doug Hutchinson), who stole forty-two million dollars from the treasury and left two guards shot dead.&lt;br /&gt;&lt;br /&gt;\"Bait\" marks the sophomore feature for Antoine Fuqua (\"The Replacement Killers\") and he handles the task fairly well even though ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>The legendary Boris Karloff ended his illustrious career by making four cheapie fright flick clunkers in Mexico. This is the token moody period Gothic horror entry from the bunch. Karloff gives a typically spry and dignified performance as Matthias Morteval, an elderly eccentric patriarch who invites several of his petty, greedy and backbiting no-count relatives to his creepy rundown castle for the reading of a will. Pretty soon the hateful guests are getting bumped off by lethal life-sized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>This tale based on two Edgar Allen Poe pieces (\"The Fall of the House of Usher\", \"Dance of Death\" (poem) ) is actually quite creepy from beginning to end. It is similar to some of the old black-and-white movies about people that meet in an old decrepit house (for example, \"The Cat and the Canary\", \"The Old Dark House\", \"Night of Terror\" and so on). Boris Karloff plays a demented inventor of life-size dolls that terrorize the guests. He dies early in the film (or does he ? ) and the residents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Aro Tolbukhin burnt alive seven people in a Mission in Guatemala in the 70's. Also he declared that he had murdered another 16 people (he used to kill pregnant women, and then he set them on fire).&lt;br /&gt;&lt;br /&gt;This movie is a documentary that portraits the personality of Aro through several interviews with people that got to know him and through some scenes played by actors based on real facts.&lt;br /&gt;&lt;br /&gt;\"Aro Tolbukhin\" is a serious work, so analytical, it's not morbid at all. Such a horrify...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>After seeing several movies of Villaronga, I had a pretty clear opinion about him -- he concentrates too much on the personal aspect of the characters, forgetting about a rhythm of the movie. That is why, though having good critics, his movies never caught the broad audience attention. In ARo he follows the same line, but really improved on the rhythm, especially in the end of the movie. Frankly speaking, I slept through the first part, cause though the first part gives necessary information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>There's so many things to fall for in Aro Tolbukhin. En la mente del asesino (Inside the killer's mind), that it's very hard to talk about it without giving any kind of warning. Let's just say that this movie is like an exercise in cinema but really, really great done. ItÂ´s made with super 8, black and white shots, 35 milimeters, color, interviews, flashbacks. Aro Tolbukhin itÂ´s like a movie made a documental or viceversa, which most peculiar aspect relays on the doubt that leaves you wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>This has got to be a unique twists of two genres of ever seen. The giant monster movie genre with the living mummy movie genre. This unique blend makes for a unique and compelling story. The casts is outstanding, including TOM BOSLEY who as far as I know never has been in a horror movie before, ever. The effects are impressive and the idea of a giant mummy filled with smaller mummies is a cool one. My one complaint, I just wish we saw more of the giant mummy, but other then that I think they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>Without Kirsten Miller this project needn't have been completed. However with the awe inspiring beauty and talent that is Miss Miller I would definitely recommend it. It looked as if the other actors were only playing to her strong performance. Wagner's dismal attempt to honor this film was a bit disappointing, but his few scenes didn't detract from being entertained. Mostly my criticisms are with the writing and plot line, the group of talent assembled did a heroic job of salvaging what sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>Sure, Titanic was a good movie, the first time you see it, but you really should see it a second time and your opinion of the film will definetly change. The first time you see the movie you see the underlying love-story and think: ooh, how romantic. The second time (and I am not the only one to think this) it is just annoying and you just sit there watching the movie thinking, When is this d**n ship going to sink??? And even this is not as impressive when you see it several times. The actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>When I saw this movie I was stunned by what a great movie it was. This is the only movie I think I would ever give a 10 star rating. I am sure this movie will always be in my top 5.&lt;br /&gt;&lt;br /&gt;The acting is superb. Leonardo DiCaprio and Kate Winslett are at their best. I don't think anyone could have a better job than Kate. &lt;br /&gt;&lt;br /&gt;If it is a rainy day and you can't decide what to rent, well, this is the one. You will love all the acting, special effects, and much much more.&lt;br /&gt;&lt;br /&gt;I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>Why do people bitch about this movie and not about awful movies like The Godfather. Titanic is the greatest movie of the 21st Century.With great acting,directing,effects,music and generally everything. This movie is always dumped by all because one day some one said they didn't like it any more so most of the world decided to agree. There is nothing wrong with this movie. All I can say is that this movie, not only being the most heavily Oscar Awarded movie of all time, the most money ever ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>Just two comments....SEVEN years apart? Hardly evidence of the film's relentless pulling-power! As has been mentioned, the low-budget telemovie status of 13 GANTRY ROW is a mitigating factor in its limited appeal. Having said that however the thing is not without merit - either as entertainment or as a fright outing per se.&lt;br /&gt;&lt;br /&gt;True, the plot at its most basic is a re-working of THE AMITYVILLE HORROR - only without much horror. More a case of intrigue! Gibney might have made a more wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>What's inexplicable? Firstly, the hatred towards this movie. It may not be the greatest movie of all time, but gimme a break, it got 11 oscars for a reason, it made EIGHTEEN HUNDRED MILLION DOLLARS for a reason. It's a damn good movie. Which brings to the other inexplicable aspect of it. I have no idea whatsoever why this movie left such an impression on me when I saw it in theaters. I've rewatched it on TV and video, and it had none of the impact it had when I saw it on the big screen (twic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>Previously, I wrote that I loved \"Titanic\", cried at its ending (many times over), and I'm a guy in his 60's. I also wondered about why this great movie, which won so many awards and was applauded by so many critics, was given only a 7.0 rating by imdb.com users.&lt;br /&gt;&lt;br /&gt;Well, I looked at the breakdown of the user ratings. While 29.0% of all votes gave it a 10 rating, 10.7% gave it a 1 rating. These 10.7% of these irrational imdb users, in effect, pulled the overall rating down to 7.0. &lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>I loved this movie since I was 7 and I saw it on the opening day. It was so touching and beautiful. I strongly recommend seeing for all. It's a movie to watch with your family by far.&lt;br /&gt;&lt;br /&gt;My MPAA rating: PG-13 for thematic elements, prolonged scenes of disastor, nudity/sexuality and some language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>The movie Titanic makes it much more then just a \"night to remember.\" It re writes a tragic history event that will always be talked about and will never been forgotten. Why so criticised? I have no idea. Could/will they ever make a movie like Titanic that is so moving and touching every time you watch it. Could they ever replace such an epic masterpiece. It will be almost impossible.&lt;br /&gt;&lt;br /&gt;The director no doubt had the major impact on the film. A simple disaster film (boring to watch) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>I had few problems with this film, and I have heard a lot of criticisms saying it is overlong and overrated. True, it is over three hours long, but I was amazed that it goes by so quickly. I don't think it is overrated at all, I think the IMDb rating is perfectly decent. The film looks sumptuous, with gorgeous costumes and excellent effects, and the direction from James Cameron rarely slips from focus. Leonardo DiCaprio gives one of his best performances as Jack, and Kate Winslet is lovely a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>I think James Cameron might be becoming my favorite director because this is my second review of his movies. Anyway, everyone remembers the RMS Titanic. It was big, fast, and \"unsinkable\"... until April 1912. It was all over the news and one of the biggest tragedies ever. Well James Cameron decided to make a movie out of it but star two fictional characters to be in the spotlight instead of the ship. Well, onto the main review but let me remind you that this is all opinion and zero fact and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic is a long but well made tragic adventure love story that takes place during the ill-fated voyage on the unsinkable ship. Writer/Director James Cameron has done a great job of making this movie about a fictional love story between two very different people and combining that with the real event of the Titanic that sunk after hitting an iceberg on April 15, 1912 claiming thousands of lives who perished in the icy freeing waters of the North Atlantic. The two leads in the film are great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>The ship may have sunk but the movie didn't!!! Director, James Cameron, from 'The Terminator' did it again with this amazing picture. One of my favorite scenes is 'The Dinner table' scene, in which Rose's family and friends meet Jack after he saves her. Rose has a look on her face that every woman should have when you meet 'THE ONE'...I hope I have that look when I am in the room with my future husband.&lt;br /&gt;&lt;br /&gt;Jack and Rose have a connection that is 'MOVIE STUFF' but it's good movie stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic has to be one of my all-time favorite movies. It has its problems (what movies don't) but still, it's enjoyable.&lt;br /&gt;&lt;br /&gt;When I stumble across someone who asks me why I like Titanic, I suppose my first reaction is \"wait a minute, you don't?\" I know so many people who don't like this movie, and I'm not saying I don't see why. \"The love story is too cheesy\" well, yes but isn't it enjoyable and moving? All right, the love story between Jack and Rose is very unrealistic, everyone know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic is a classic. I was really surprised that this movie didn't have a solid ten, overall in the IMDb user rankings. Maybe, it's just cool to not give Titanic credit nowadays, but when it was first made it was really something. When the movie came out people flocked to the theaters. When it came out on video my sister and i would watch it twice a day for a month. It was safe to say we were obsessed and for good reason. Some of the disaster scenes were hard to forgot, like the frozen baby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>Another Aussie masterpiece, this delves into the world of the unknown and the supernatural, and it does very well. It doesn't resort to the big special effects overkill like American flicks, it focuses more on emotional impact. A relatively simple plot that Rebecca Gibney &amp; Co. bring to life. It follows the story of a couple who buy an old house that was supposedly home to a very old woman who never went outside, and whose husband disappeared in mysterious circumstances a century ago. Strang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>For me personally this film goes down in my top four of all time. No exceptions. James Cameron has proved himself time and time again that he is a master storyteller. Through films such as Aliens, The Abyss and both Terminators it is clear that he was a brilliant and confidant director as far as action and science-fiction goes. He sees a story and adds a strange quality to the film. But Titanic is so much different to his other strokes of brilliance. The film is exceptionally moving and allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>Back in 1997, do I remember that year: Clinton bans cloning research, the unfortunate death of Princess Diana, the Marlins won the world series and a woman gave birth to septuplets. This was also the big year in the release of Titanic, one of the biggest films of all time: a tale about the ship of dreams, about a boy and a girl who fall in love but are torn apart by their social class and at the height of their emotional commitment the ship meets with disaster. I don't think anybody could ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>James Cameron's 'Titanic' is essentially a romantic adventure with visual grandeur and magnificence, a timeless tragic love story set against the background of this major historical event... It's an astonishing movie that exemplifies hope, love and humanity... &lt;br /&gt;&lt;br /&gt;Leonardo DiCaprio is terrific on screen with big charisma... Conveying passion, trust, insouciance and ingenuity, he's a free-spirited wanderer with artistic pretensions, and a zest for life... &lt;br /&gt;&lt;br /&gt;Kate Winslet is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>This movie re-wrote film history in every way. No one cares what anyone thinks about this movie, because it transcends criticism. Every flaw in the movie is easily overcome by the many amazing things the movie has going for it. It is an extremely beautiful movie, and I doubt many of us will see anything like it again. I've seen it more times than I care to count, and I still become transfixed every time, with a feeling which is hard to describe. One for the ages.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>Titanic directed by James Cameron presents a fictional love story on the historical setting of the Titanic. The plot is simple, noncomplicated, or not for those who love plots that twist and turn and keep you in suspense. The end of the movie can be figured out within minutes of the start of the film, but the love story is an interesting one, however. Kate Winslett is wonderful as Rose, an aristocratic young lady betrothed by Cal (Billy Zane). Early on the voyage Rose meets Jack (Leonardo Di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       1   \n",
       "3       1   \n",
       "4       1   \n",
       "5       1   \n",
       "6       1   \n",
       "7       1   \n",
       "8       1   \n",
       "9       1   \n",
       "10      1   \n",
       "11      1   \n",
       "12      1   \n",
       "13      1   \n",
       "14      1   \n",
       "15      1   \n",
       "16      1   \n",
       "17      1   \n",
       "18      1   \n",
       "19      1   \n",
       "20      1   \n",
       "21      1   \n",
       "22      1   \n",
       "23      1   \n",
       "24      1   \n",
       "25      1   \n",
       "26      1   \n",
       "27      1   \n",
       "28      1   \n",
       "29      1   \n",
       "30      1   \n",
       "31      1   \n",
       "32      1   \n",
       "33      1   \n",
       "34      1   \n",
       "35      1   \n",
       "36      1   \n",
       "37      1   \n",
       "38      1   \n",
       "39      1   \n",
       "40      1   \n",
       "41      1   \n",
       "42      1   \n",
       "43      1   \n",
       "44      1   \n",
       "45      1   \n",
       "46      1   \n",
       "47      1   \n",
       "48      1   \n",
       "49      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \n",
       "0   Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which...  \n",
       "1   Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if yo...  \n",
       "2   Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic...  \n",
       "3   This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher K...  \n",
       "4   This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his ch...  \n",
       "5   This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's characte...  \n",
       "6   Yes its an art... to successfully make a slow paced thriller.<br /><br />The story unfolds in nice volumes while you don't even notice it happening.<br /><br />Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of sus...  \n",
       "7   In this \"critically acclaimed psychological thriller based on true events, Gabriel (Robin Williams), a celebrated writer and late-night talk show host, becomes captivated by the harrowing story of a young listener and his adoptive mother (Toni Collette). When troubling questions arise about this boy's (story), however, Gabriel finds himself drawn into a widening mystery that hides a deadly secret\n",
       "\" according to film's official synopsis.<br /><br />You really should STOP reading these comment...  \n",
       "8   THE NIGHT LISTENER (2006) **1/2 Robin Williams, Toni Collette, Bobby Cannavale, Rory Culkin, Joe Morton, Sandra Oh, John Cullum, Lisa Emery, Becky Ann Baker. (Dir: Patrick Stettner) <br /><br />Hitchcockian suspenser gives Williams a stand-out low-key performance.<br /><br />What is it about celebrities and fans? What is the near paranoia one associates with the other and why is it almost the norm? <br /><br />In the latest derange fan scenario, based on true events no less, Williams stars a...  \n",
       "9   You know, Robin Williams, God bless him, is constantly shooting himself in the foot lately with all these dumb comedies he has done this decade (with perhaps the exception of \"Death To Smoochy\", which bombed when it came out but is now a cult classic). The dramas he has made lately have been fantastic, especially \"Insomnia\" and \"One Hour Photo\". \"The Night Listener\", despite mediocre reviews and a quick DVD release, is among his best work, period.<br /><br />This is a very chilling story, ev...  \n",
       "10  When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle...  \n",
       "11                            I liked the film. Some of the action scenes were very interesting, tense and well done. I especially liked the opening scene which had a semi truck in it. A very tense action scene that seemed well done.<br /><br />Some of the transitional scenes were filmed in interesting ways such as time lapse photography, unusual colors, or interesting angles. Also the film is funny is several parts. I also liked how the evil guy was portrayed too. I'd give the film an 8 out of 10.  \n",
       "12  There are many illnesses born in the mind of man which have been given life in modern times. Constant vigilance or accrued information in the realm of Pyschosis, have kept psychologists, counselors and psychiatrists busy with enough work to last them decades. Occasionally, some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence. That is the premise of the film entitled \" The Night Listner.\" It tells the story of a popular radio host ...  \n",
       "13  I enjoyed The Night Listener very much. It's one of the better movies of the summer.<br /><br />Robin Williams gives one of his best performances. In fact, the entire cast was very good. All played just the right notes for their characters - not too much and not too little. Sandra Oh adds a wonderful comic touch. Toni Collette is great as the Mom, and never goes over the top. Everyone is very believable.<br /><br />It's a short movie, just under an hour and a half. I noticed the general rele...  \n",
       "14  The Night Listener is probably not one of William's best roles, but he makes a very interesting character in a somewhat odd but very different movie. I can guarantee you that you have never seen this kind of movie before. Some people maybe won't like the slow pacing of this movie, but I think it's the great plus of the movie. It is definitely one of the top movies that have come out the year 2006. It has a intriguing performance in a movie with a great content, dramatic feeling. This is no a...  \n",
       "15  Like one of the previous commenters said, this had the foundations of a great movie but something happened on the way to delivery. Such a waste because Collette's performance was eerie and Williams was believable. I just kept waiting for it to get better. I don't think it was bad editing or needed another director, it could have just been the film. It came across as a Canadian movie, something like the first few seasons of X-Files. Not cheap, just hokey. Also, it needed a little more suspens...  \n",
       "16  The Night Listener held my attention, with Robin Williams shining as a New York City radio host who becomes enamored with his friendship with a 14 year old boy (Rory Culkin) who is very ill. Williams has never met the boy in person, as they have only been in contact by talking on the telephone. However, Williams' ex-boyfriend (nice job from Bobby Cannavale) raises doubt about the boy, which prompts Williams to arrange a meeting with him in person. What follows makes a permanent impact on Wil...  \n",
       "17  Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tones) becomes acquainted and friends with a fourteen-year-old boy from Wisconsin named Pete Logand(Rory Culkin),who has written a book detailing sexual abuse from his parents. To boot,Pete has AIDS and this compels Gabriel further still,since his partner Jess(Bobby Cannavale,good)happens to be a survivor of HIV himself. <br /><br />He also acquaints himself with Pete's guardian,a woman named Donn...  \n",
       "18  If there is one thing to recommend about this film is that it is intriguing. The premise certainly draws the audience in because it is a mystery, and throughout the film there are hints that there is something dark lurking about. However, there is not much tension, and Williams' mild mannered portrayal doesn't do much to makes us relate to his obsession with the boy.<br /><br />Collete fares much better as the woman whose true nature and intentions are not very clear. The production felt rus...  \n",
       "19  I absolutely LOVED this film! I do not at all relate to all the other comments I have read about it. I was COMPLETELY enthralled through every second! <br /><br />I found the story gripping, the acting intense, and the direction spot-on. I would literally jump every time the phone would ring close to the end of the movie. Even though there was nothing \"scary\" about the story itself, I was soundly on edge through the whole movie - and for the rest of my evening. <br /><br />I found that there...  \n",
       "20  This Night Listener is better than people are generally saying. It has weaknesses, and it seems to be having a genre identity crisis, no doubt, but I think its creepy atmosphere and intriguing performances make up for this. The whole thing feels like one of those fireside \"this happened to a friend of a friend of mine\" ghost stories. One big complaint about the movie is the pacing: but the slow and sometimes awkward pacing is deliberate. Everything that unfolds in this movie is kept well wit...  \n",
       "21  When it comes to movies I can be pretty picky, and I'll complain about anything and everything that is done wrong. While every movie has its flaws, The Night Listener had an exceptionally low count.<br /><br />If you read the last review (it was hard, since half of it was written in caps and it contained no actual information about the movie), you may have been led to believe that this movie was not too well done. Unfortunately, if you read more than 3 lines into that same review, you discov...  \n",
       "22  Somewhat funny and well-paced action thriller that has Jamie Foxx as a hapless, fast-talking hoodlum who is chosen by an overly demanding U.S. Treasury Agent (David Morse) to be released on the streets of New York to find a picky computer thief/hacker (Doug Hutchinson), who stole forty-two million dollars from the treasury and left two guards shot dead.<br /><br />\"Bait\" marks the sophomore feature for Antoine Fuqua (\"The Replacement Killers\") and he handles the task fairly well even though ...  \n",
       "23  The legendary Boris Karloff ended his illustrious career by making four cheapie fright flick clunkers in Mexico. This is the token moody period Gothic horror entry from the bunch. Karloff gives a typically spry and dignified performance as Matthias Morteval, an elderly eccentric patriarch who invites several of his petty, greedy and backbiting no-count relatives to his creepy rundown castle for the reading of a will. Pretty soon the hateful guests are getting bumped off by lethal life-sized ...  \n",
       "24  This tale based on two Edgar Allen Poe pieces (\"The Fall of the House of Usher\", \"Dance of Death\" (poem) ) is actually quite creepy from beginning to end. It is similar to some of the old black-and-white movies about people that meet in an old decrepit house (for example, \"The Cat and the Canary\", \"The Old Dark House\", \"Night of Terror\" and so on). Boris Karloff plays a demented inventor of life-size dolls that terrorize the guests. He dies early in the film (or does he ? ) and the residents...  \n",
       "25  Aro Tolbukhin burnt alive seven people in a Mission in Guatemala in the 70's. Also he declared that he had murdered another 16 people (he used to kill pregnant women, and then he set them on fire).<br /><br />This movie is a documentary that portraits the personality of Aro through several interviews with people that got to know him and through some scenes played by actors based on real facts.<br /><br />\"Aro Tolbukhin\" is a serious work, so analytical, it's not morbid at all. Such a horrify...  \n",
       "26  After seeing several movies of Villaronga, I had a pretty clear opinion about him -- he concentrates too much on the personal aspect of the characters, forgetting about a rhythm of the movie. That is why, though having good critics, his movies never caught the broad audience attention. In ARo he follows the same line, but really improved on the rhythm, especially in the end of the movie. Frankly speaking, I slept through the first part, cause though the first part gives necessary information...  \n",
       "27  There's so many things to fall for in Aro Tolbukhin. En la mente del asesino (Inside the killer's mind), that it's very hard to talk about it without giving any kind of warning. Let's just say that this movie is like an exercise in cinema but really, really great done. ItÂ´s made with super 8, black and white shots, 35 milimeters, color, interviews, flashbacks. Aro Tolbukhin itÂ´s like a movie made a documental or viceversa, which most peculiar aspect relays on the doubt that leaves you wonder...  \n",
       "28  This has got to be a unique twists of two genres of ever seen. The giant monster movie genre with the living mummy movie genre. This unique blend makes for a unique and compelling story. The casts is outstanding, including TOM BOSLEY who as far as I know never has been in a horror movie before, ever. The effects are impressive and the idea of a giant mummy filled with smaller mummies is a cool one. My one complaint, I just wish we saw more of the giant mummy, but other then that I think they...  \n",
       "29  Without Kirsten Miller this project needn't have been completed. However with the awe inspiring beauty and talent that is Miss Miller I would definitely recommend it. It looked as if the other actors were only playing to her strong performance. Wagner's dismal attempt to honor this film was a bit disappointing, but his few scenes didn't detract from being entertained. Mostly my criticisms are with the writing and plot line, the group of talent assembled did a heroic job of salvaging what sho...  \n",
       "30  Sure, Titanic was a good movie, the first time you see it, but you really should see it a second time and your opinion of the film will definetly change. The first time you see the movie you see the underlying love-story and think: ooh, how romantic. The second time (and I am not the only one to think this) it is just annoying and you just sit there watching the movie thinking, When is this d**n ship going to sink??? And even this is not as impressive when you see it several times. The actin...  \n",
       "31  When I saw this movie I was stunned by what a great movie it was. This is the only movie I think I would ever give a 10 star rating. I am sure this movie will always be in my top 5.<br /><br />The acting is superb. Leonardo DiCaprio and Kate Winslett are at their best. I don't think anyone could have a better job than Kate. <br /><br />If it is a rainy day and you can't decide what to rent, well, this is the one. You will love all the acting, special effects, and much much more.<br /><br />I...  \n",
       "32  Why do people bitch about this movie and not about awful movies like The Godfather. Titanic is the greatest movie of the 21st Century.With great acting,directing,effects,music and generally everything. This movie is always dumped by all because one day some one said they didn't like it any more so most of the world decided to agree. There is nothing wrong with this movie. All I can say is that this movie, not only being the most heavily Oscar Awarded movie of all time, the most money ever ma...  \n",
       "33  Just two comments....SEVEN years apart? Hardly evidence of the film's relentless pulling-power! As has been mentioned, the low-budget telemovie status of 13 GANTRY ROW is a mitigating factor in its limited appeal. Having said that however the thing is not without merit - either as entertainment or as a fright outing per se.<br /><br />True, the plot at its most basic is a re-working of THE AMITYVILLE HORROR - only without much horror. More a case of intrigue! Gibney might have made a more wo...  \n",
       "34  What's inexplicable? Firstly, the hatred towards this movie. It may not be the greatest movie of all time, but gimme a break, it got 11 oscars for a reason, it made EIGHTEEN HUNDRED MILLION DOLLARS for a reason. It's a damn good movie. Which brings to the other inexplicable aspect of it. I have no idea whatsoever why this movie left such an impression on me when I saw it in theaters. I've rewatched it on TV and video, and it had none of the impact it had when I saw it on the big screen (twic...  \n",
       "35  Previously, I wrote that I loved \"Titanic\", cried at its ending (many times over), and I'm a guy in his 60's. I also wondered about why this great movie, which won so many awards and was applauded by so many critics, was given only a 7.0 rating by imdb.com users.<br /><br />Well, I looked at the breakdown of the user ratings. While 29.0% of all votes gave it a 10 rating, 10.7% gave it a 1 rating. These 10.7% of these irrational imdb users, in effect, pulled the overall rating down to 7.0. <b...  \n",
       "36                                                                                                                                                                                                    I loved this movie since I was 7 and I saw it on the opening day. It was so touching and beautiful. I strongly recommend seeing for all. It's a movie to watch with your family by far.<br /><br />My MPAA rating: PG-13 for thematic elements, prolonged scenes of disastor, nudity/sexuality and some language.  \n",
       "37  The movie Titanic makes it much more then just a \"night to remember.\" It re writes a tragic history event that will always be talked about and will never been forgotten. Why so criticised? I have no idea. Could/will they ever make a movie like Titanic that is so moving and touching every time you watch it. Could they ever replace such an epic masterpiece. It will be almost impossible.<br /><br />The director no doubt had the major impact on the film. A simple disaster film (boring to watch) ...  \n",
       "38  I had few problems with this film, and I have heard a lot of criticisms saying it is overlong and overrated. True, it is over three hours long, but I was amazed that it goes by so quickly. I don't think it is overrated at all, I think the IMDb rating is perfectly decent. The film looks sumptuous, with gorgeous costumes and excellent effects, and the direction from James Cameron rarely slips from focus. Leonardo DiCaprio gives one of his best performances as Jack, and Kate Winslet is lovely a...  \n",
       "39  I think James Cameron might be becoming my favorite director because this is my second review of his movies. Anyway, everyone remembers the RMS Titanic. It was big, fast, and \"unsinkable\"... until April 1912. It was all over the news and one of the biggest tragedies ever. Well James Cameron decided to make a movie out of it but star two fictional characters to be in the spotlight instead of the ship. Well, onto the main review but let me remind you that this is all opinion and zero fact and ...  \n",
       "40  Titanic is a long but well made tragic adventure love story that takes place during the ill-fated voyage on the unsinkable ship. Writer/Director James Cameron has done a great job of making this movie about a fictional love story between two very different people and combining that with the real event of the Titanic that sunk after hitting an iceberg on April 15, 1912 claiming thousands of lives who perished in the icy freeing waters of the North Atlantic. The two leads in the film are great...  \n",
       "41  The ship may have sunk but the movie didn't!!! Director, James Cameron, from 'The Terminator' did it again with this amazing picture. One of my favorite scenes is 'The Dinner table' scene, in which Rose's family and friends meet Jack after he saves her. Rose has a look on her face that every woman should have when you meet 'THE ONE'...I hope I have that look when I am in the room with my future husband.<br /><br />Jack and Rose have a connection that is 'MOVIE STUFF' but it's good movie stuf...  \n",
       "42  Titanic has to be one of my all-time favorite movies. It has its problems (what movies don't) but still, it's enjoyable.<br /><br />When I stumble across someone who asks me why I like Titanic, I suppose my first reaction is \"wait a minute, you don't?\" I know so many people who don't like this movie, and I'm not saying I don't see why. \"The love story is too cheesy\" well, yes but isn't it enjoyable and moving? All right, the love story between Jack and Rose is very unrealistic, everyone know...  \n",
       "43  Titanic is a classic. I was really surprised that this movie didn't have a solid ten, overall in the IMDb user rankings. Maybe, it's just cool to not give Titanic credit nowadays, but when it was first made it was really something. When the movie came out people flocked to the theaters. When it came out on video my sister and i would watch it twice a day for a month. It was safe to say we were obsessed and for good reason. Some of the disaster scenes were hard to forgot, like the frozen baby...  \n",
       "44  Another Aussie masterpiece, this delves into the world of the unknown and the supernatural, and it does very well. It doesn't resort to the big special effects overkill like American flicks, it focuses more on emotional impact. A relatively simple plot that Rebecca Gibney & Co. bring to life. It follows the story of a couple who buy an old house that was supposedly home to a very old woman who never went outside, and whose husband disappeared in mysterious circumstances a century ago. Strang...  \n",
       "45  For me personally this film goes down in my top four of all time. No exceptions. James Cameron has proved himself time and time again that he is a master storyteller. Through films such as Aliens, The Abyss and both Terminators it is clear that he was a brilliant and confidant director as far as action and science-fiction goes. He sees a story and adds a strange quality to the film. But Titanic is so much different to his other strokes of brilliance. The film is exceptionally moving and allo...  \n",
       "46  Back in 1997, do I remember that year: Clinton bans cloning research, the unfortunate death of Princess Diana, the Marlins won the world series and a woman gave birth to septuplets. This was also the big year in the release of Titanic, one of the biggest films of all time: a tale about the ship of dreams, about a boy and a girl who fall in love but are torn apart by their social class and at the height of their emotional commitment the ship meets with disaster. I don't think anybody could ha...  \n",
       "47  James Cameron's 'Titanic' is essentially a romantic adventure with visual grandeur and magnificence, a timeless tragic love story set against the background of this major historical event... It's an astonishing movie that exemplifies hope, love and humanity... <br /><br />Leonardo DiCaprio is terrific on screen with big charisma... Conveying passion, trust, insouciance and ingenuity, he's a free-spirited wanderer with artistic pretensions, and a zest for life... <br /><br />Kate Winslet is a...  \n",
       "48                                  This movie re-wrote film history in every way. No one cares what anyone thinks about this movie, because it transcends criticism. Every flaw in the movie is easily overcome by the many amazing things the movie has going for it. It is an extremely beautiful movie, and I doubt many of us will see anything like it again. I've seen it more times than I care to count, and I still become transfixed every time, with a feeling which is hard to describe. One for the ages.  \n",
       "49  Titanic directed by James Cameron presents a fictional love story on the historical setting of the Titanic. The plot is simple, noncomplicated, or not for those who love plots that twist and turn and keep you in suspense. The end of the movie can be figured out within minutes of the start of the film, but the love story is an interesting one, however. Kate Winslett is wonderful as Rose, an aristocratic young lady betrothed by Cal (Billy Zane). Early on the voyage Rose meets Jack (Leonardo Di...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d5b27a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([t['label'] for t in train]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b2c87a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bd5b0448294617bf2006aad509c6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923541a2775244e7821b766cdb1ca8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b71772b9b64a43baa834b192a25a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166af9a33ae64c2b9478e0b8172315ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d182ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train es un anfibio lista - diccionario! muy bueno :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e87f8fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': \"Sure, Titanic was a good movie, the first time you see it, but you really should see it a second time and your opinion of the film will definetly change. The first time you see the movie you see the underlying love-story and think: ooh, how romantic. The second time (and I am not the only one to think this) it is just annoying and you just sit there watching the movie thinking, When is this d**n ship going to sink??? And even this is not as impressive when you see it several times. The acting in this film is not bad, but definetly not great either. Was I glad DiCaprio did not win an oscar for that film, I mean who does he think he is, Anthony Hopkins or Denzel Washington? He does 1 half-good movie and won't do a film for less than $20 million. And then everyone is suprised that there are hardly any films with him in it. But enough about, in my eyes, the worst character of the film. Kate Winslet's performance on the other hand was wonderful. I also tink that the director is very talented to put a film of such a magnitude together. There is one lesson to be learned about this movie: there are too many love-stories as it is, filmmakers shouldn't try to add a crummy romance in to every single movie!!! Out of a possible 100% I give this film a mere 71%.\"}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4a0c743e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "41c4f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55fdae9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8046afd5766940a18792a4f6cb9ba903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac3002852424dd79bc45869b7daa61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedfb904ca4d4d3db5a54bdd28a42330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "25e54449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estaria bueno no haber tokenizado el unsupervised creo\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "db5ffc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03df7c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dfcdd1fd6b45349b0108422a170817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a58bc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "23cfdb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5623b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.1, inplace=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())[217]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d11191e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())[216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0cd382c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())[215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c1a20a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8fc7e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\"/media/dataista/DATA/my-transformers/first_trainer\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5a55d48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.82 GiB total capacity; 1.58 GiB already allocated; 68.00 MiB free; 1.62 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1503\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.82 GiB total capacity; 1.58 GiB already allocated; 68.00 MiB free; 1.62 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28b2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De nuevo, todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75c4541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Loading cached processed dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-16e01676e45a188f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a073de04b644538bfcdb4e90c42aa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-8056582c24960265.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached shuffled indices for dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-f57ecca1b0a2e333.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "raw_datasets = datasets.load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "#full_train_dataset = tokenized_datasets[\"train\"]\n",
    "#full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"/media/dataista/DATA/my-transformers/first_trainer\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbafc49",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.82 GiB total capacity; 1.49 GiB already allocated; 177.62 MiB free; 1.52 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1503\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 3.82 GiB total capacity; 1.49 GiB already allocated; 177.62 MiB free; 1.52 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977d1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El problema no es el dataset, sino BERT. Pareceria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b2eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7507c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigue sin funcionar con DistillBert y 10 samples\n",
    "# Pruebo con Batch size y sino pasamos a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4486aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Loading cached processed dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-720c17e77000dab8.arrow\n",
      "Loading cached processed dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-9d8983fb05751a6e.arrow\n",
      "Loading cached processed dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-c86709e7338e7405.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached shuffled indices for dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-71f2b545007cd7bd.arrow\n",
      "Loading cached shuffled indices for dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-484a3647a3e90554.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-cased\"\n",
    "\n",
    "\n",
    "raw_datasets = datasets.load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10))\n",
    "#full_train_dataset = tokenized_datasets[\"train\"]\n",
    "#full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"/media/dataista/DATA/my-transformers/first_trainer\",\n",
    "                                 per_device_train_batch_size=1,\n",
    "                                 per_device_eval_batch_size=1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffb1f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.6969985961914062, metrics={'train_runtime': 5.2253, 'train_samples_per_second': 5.741, 'train_steps_per_second': 5.741, 'total_flos': 6062565150720.0, 'train_loss': 0.6969985961914062, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5627ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10))\n",
    "#small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3757f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"/media/dataista/DATA/my-transformers/first_trainer\",\n",
    "                                 per_device_train_batch_size=1,\n",
    "                                 per_device_eval_batch_size=1,\n",
    "                                 no_cuda=True)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52db8a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.459854793548584, metrics={'train_runtime': 30.7567, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.975, 'total_flos': 6062565150720.0, 'train_loss': 0.459854793548584, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7b296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"/media/dataista/DATA/my-transformers/first_trainer\",\n",
    "                                 per_device_train_batch_size=8,\n",
    "                                 per_device_eval_batch_size=8,\n",
    "                                 no_cuda=True)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a85a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.1051294207572937, metrics={'train_runtime': 23.2904, 'train_samples_per_second': 1.288, 'train_steps_per_second': 0.258, 'total_flos': 6062565150720.0, 'train_loss': 0.1051294207572937, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b316974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-71f2b545007cd7bd.arrow\n",
      "Loading cached shuffled indices for dataset at /home/dataista/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-484a3647a3e90554.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 01:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=0.4871863410586402, metrics={'train_runtime': 113.4901, 'train_samples_per_second': 1.322, 'train_steps_per_second': 0.185, 'total_flos': 30312825753600.0, 'train_loss': 0.4871863410586402, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ec189",
   "metadata": {},
   "source": [
    "# Nos vamos a la nube\n",
    "\n",
    "* [GPUS en Google Cloud](https://cloud.google.com/compute/docs/gpus)\n",
    "* [Crear instancia de Google Cloud con GPU attacheada](https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
